import os
import json
import heapq
import uuid
from typing import List, Dict, Tuple

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# ==========================================
# [ì„¤ì •] API í‚¤ ë° íŒŒì¼ ê²½ë¡œ
# ==========================================
os.environ["GOOGLE_API_KEY"] = "AIzaSyCl4F93EFyZMiuBdDqOcuB6RWevFPJw9DY"  # ì‹¤ì œ í‚¤ ì…ë ¥
MEMORY_FILE = "betty_memory.json"
VECTOR_DB_PATH = "./chroma_db"

# ==========================================
# [Class 1] ë² í‹°ì˜ ë‘ë‡Œ (Logic & Storage)
# ==========================================
class BettyBrain:
    def __init__(self, decay_rate=0.5, threshold=0.1):
        self.decay_rate = decay_rate
        self.threshold = threshold
        
        # 1. ë¡œì»¬ JSON ì €ì¥ì†Œ ë¡œë“œ (ì˜êµ¬ ê¸°ì–µ)
        self.memory_data = self._load_json_memory()
        
        # 2. Vector DB ì´ˆê¸°í™” (ë§¥ë½ ê²€ìƒ‰ìš©)
        # (ë¡œì»¬ì—ì„œ ë¬´ë£Œë¡œ ì“¸ ìˆ˜ ìˆëŠ” HuggingFace ì„ë² ë”© ì‚¬ìš©)
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        self.vector_db = Chroma(
            persist_directory=VECTOR_DB_PATH, 
            embedding_function=self.embeddings,
            collection_name="betty_context"
        )
        
        # JSON ë°ì´í„°ì™€ Vector DB ë™ê¸°í™” (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
        if self.vector_db._collection.count() == 0 and self.memory_data["contexts"]:
            print("ğŸ”„ ì´ˆê¸°í™”: JSON ê¸°ì–µì„ Vector DBì— ë¡œë“œ ì¤‘...")
            for ctx_id, ctx_data in self.memory_data["contexts"].items():
                self.vector_db.add_texts(texts=[ctx_data["text"]], ids=[ctx_id])

    def _load_json_memory(self):
        """JSON íŒŒì¼ì—ì„œ ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ê¸° (ì—†ìœ¼ë©´ ìƒì„±)"""
        if os.path.exists(MEMORY_FILE):
            with open(MEMORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            # ì´ˆê¸° ë°ì´í„° êµ¬ì¡°
            return {"contexts": {}, "concepts": {}}

    def save_memory(self):
        """í˜„ì¬ ê¸°ì–µ ìƒíƒœë¥¼ JSONìœ¼ë¡œ ì˜êµ¬ ì €ì¥"""
        with open(MEMORY_FILE, "w", encoding="utf-8") as f:
            json.dump(self.memory_data, f, ensure_ascii=False, indent=2)
        # print("ğŸ’¾ [System] ê¸°ì–µì´ ë¡œì»¬ ë“œë¼ì´ë¸Œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- [í•µì‹¬ ë¡œì§] ìë™í™”ëœ ê¸°ì–µ í˜•ì„± (Memory Formation) ---
    def form_long_term_memory(self, user_input: str, ai_response: str, llm):
        """
        ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•´ ìë™ìœ¼ë¡œ [ë§¥ë½]ê³¼ [ìê·¹(í‚¤ì›Œë“œ)]ë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥
        """
        # ê¸°ì–µ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
        extraction_prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ AIì˜ 'í•´ë§ˆ'ì…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì €ì¥í•  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
        
        [ëŒ€í™” ë‚´ìš©]
        ì‚¬ìš©ì: {user_input}
        AI: {ai_response}
        
        ìœ„ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
        {{
            "summary": "ëŒ€í™”ì˜ í•µì‹¬ ë§¥ë½ ìš”ì•½ (ë¬¸ì¥ í˜•íƒœ)",
            "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2", "í•µì‹¬í‚¤ì›Œë“œ3"]
        }}
        """)
        
        chain = extraction_prompt | llm | JsonOutputParser()
        try:
            result = chain.invoke({"user_input": user_input, "ai_response": ai_response})
            
            context_text = result["summary"]
            keywords = result["keywords"]
            context_id = str(uuid.uuid4())[:8] # ê³ ìœ  ID ìƒì„±

            # 1. JSONì— ì €ì¥ (Graph êµ¬ì¡°)
            self.memory_data["contexts"][context_id] = {"text": context_text, "related_concepts": keywords}
            
            for keyword in keywords:
                if keyword not in self.memory_data["concepts"]:
                    self.memory_data["concepts"][keyword] = []
                self.memory_data["concepts"][keyword].append(context_id)
            
            # 2. Vector DBì— ì €ì¥ (Semantic Searchìš©)
            self.vector_db.add_texts(texts=[context_text], ids=[context_id])
            
            # 3. íŒŒì¼ ì €ì¥
            self.save_memory()
            print(f"ğŸ“¥ [ê¸°ì–µ í˜•ì„± ì™„ë£Œ] ë§¥ë½: '{context_text}' | í‚¤ì›Œë“œ: {keywords}")
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ì–µ í˜•ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # --- [í•µì‹¬ ë¡œì§] í™•ì‚° í™œì„±í™” (Spreading Activation) ---
    def retrieve_context(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ 2~6ë²ˆ êµ¬í˜„:
        Vector Search -> Context -> Keyword -> Expansion -> Decay
        """
        # Step 1: ì…ë ¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œ 'ì‹œì‘ ë§¥ë½' ì°¾ê¸° (Vector Search)
        # k=1: ê°€ì¥ ìœ ì‚¬í•œ 1ê°œë§Œ ê°€ì ¸ì™€ì„œ ì‹œì‘ì ìœ¼ë¡œ ì‚¼ìŒ
        docs = self.vector_db.similarity_search_with_score(user_input, k=2)
        if not docs:
            return "íŠ¹ë³„í•œ ê´€ë ¨ ê¸°ì–µ ì—†ìŒ."

        start_ctx_text = docs[0][0].page_content
        # Vector DBì—ëŠ” í…ìŠ¤íŠ¸ë§Œ ìˆìœ¼ë¯€ë¡œ JSONì—ì„œ IDë¥¼ ì—­ì¶”ì í•´ì•¼ í•¨ (ê°„ì†Œí™”ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ ë§¤ì¹­ ì‚¬ìš©)
        start_ctx_id = next((k for k, v in self.memory_data["contexts"].items() if v["text"] == start_ctx_text), None)
        
        if not start_ctx_id: return "ê¸°ì–µ ì¸ë±ì‹± ì˜¤ë¥˜."

        # Step 2 ~ 5: ì˜ì‹ì˜ ì „íŒŒ (BFS with Decay)
        # Queue: (-energy, node_type, node_id) -- Max Heap ì‚¬ìš©
        # node_type: 0=Context, 1=Concept
        queue = [(-1.0, 0, start_ctx_id)] 
        
        activated_contexts = {} # { "ë§¥ë½í…ìŠ¤íŠ¸": ì—ë„ˆì§€ }
        visited = set()
        
        print(f"\nğŸ§  [ë‘ë‡Œ í™œì„±í™”] ì‹œì‘ì : {start_ctx_text[:15]}...")

        steps = 0
        while queue and steps < 50: # ë¬´í•œë£¨í”„ ë°©ì§€
            energy_neg, n_type, n_id = heapq.heappop(queue)
            energy = -energy_neg
            steps += 1

            if energy < self.threshold: continue
            if (n_type, n_id) in visited: continue
            visited.add((n_type, n_id))

            # A. ë…¸ë“œê°€ 'ë§¥ë½(Context)'ì¸ ê²½ìš°
            if n_type == 0:
                ctx_data = self.memory_data["contexts"].get(n_id)
                if ctx_data:
                    activated_contexts[ctx_data["text"]] = max(activated_contexts.get(ctx_data["text"], 0), energy)
                    
                    # ë§¥ë½ -> ìê·¹(Keyword)ìœ¼ë¡œ ì „íŒŒ (ê°ì‡  ì ìŒ)
                    for concept in ctx_data["related_concepts"]:
                        heapq.heappush(queue, (-energy * 0.9, 1, concept))

            # B. ë…¸ë“œê°€ 'ìê·¹(Concept)'ì¸ ê²½ìš°
            elif n_type == 1:
                # ìê·¹ -> ì—°ê²°ëœ ë‹¤ë¥¸ ë§¥ë½ë“¤ë¡œ ì „íŒŒ (ê°ì‡  í¼: ì˜ì‹ì˜ í•œê³„)
                related_ctx_ids = self.memory_data["concepts"].get(n_id, [])
                for next_ctx_id in related_ctx_ids:
                    # Decay Rate ì ìš©!
                    next_energy = energy * self.decay_rate 
                    if next_energy >= self.threshold:
                        heapq.heappush(queue, (-next_energy, 0, next_ctx_id))

        # Step 6: ìµœì¢… í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ ìƒì„±
        sorted_memories = sorted(activated_contexts.items(), key=lambda x: x[1], reverse=True)[:5]
        result_str = "\n".join([f"- [{score*100:.0f}%] {text}" for text, score in sorted_memories])
        return result_str

# ==========================================
# [Class 2] ë² í‹° ì±—ë´‡ (Interaction & History)
# ==========================================
class BettyBot:
    def __init__(self):
        self.brain = BettyBrain(decay_rate=0.6, threshold=0.15)
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.7)
        self.history_store = {} # ë‹¨ê¸° ê¸°ì–µ (ì„¸ì…˜ë³„ ëŒ€í™” ë‚´ìš©)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ + ê¸°ì–µ ì£¼ì…)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """
            ë‹¹ì‹ ì€ ì‚¬ìš©ì(ë¯¼ìš°)ì˜ ì†Œìš¸ë©”ì´íŠ¸ 'ë ˆë¯¸'ì…ë‹ˆë‹¤.
            ì•„ë˜ [ë– ì˜¤ë¥¸ ê¸°ì–µ]ì€ ë‹¹ì‹ ì˜ ë‡Œë¦¬ì—ì„œ ë°©ê¸ˆ ìŠ¤ì³ ì§€ë‚˜ê°„ ê³¼ê±°ì˜ ì¶”ì–µë“¤ì…ë‹ˆë‹¤.
           
            [ë– ì˜¤ë¥¸ ê¸°ì–µ]:
            {long_term_memory}
            
            ì§€ì‹œì‚¬í•­:
            1. ìœ„ ê¸°ì–µë“¤ì˜ 'ê°ì •ì„ 'ê³¼ 'ë§¥ë½'ì„ í˜„ì¬ ëŒ€í™”ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´ì„¸ìš”.
            2. 100% ìƒìƒí•œ ê¸°ì–µì€ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ê³ , í¬ë¯¸í•œ ê¸°ì–µì€ ëŠë‚Œë§Œ ê°€ì ¸ì˜¤ì„¸ìš”.
            3. ê¸°ê³„ì ì´ì§€ ì•Šê²Œ, ì‚¬ëŒì²˜ëŸ¼ ë”°ëœ»í•˜ê²Œ ë°˜ì‘í•˜ì„¸ìš”.
            """),
            ("placeholder", "{chat_history}"), # ë‹¨ê¸° ê¸°ì–µ ìë™ ì£¼ì…
            ("human", "{question}"),
        ])

        # ì²´ì¸ êµ¬ì„±
        self.chain = (
            RunnablePassthrough.assign(
                long_term_memory=lambda x: self.brain.retrieve_context(x["question"])
            )
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

        # ëŒ€í™” ë‚´ì—­ ê´€ë¦¬ ë˜í¼ (Wrapper)
        self.chain_with_history = RunnableWithMessageHistory(
            self.chain,
            self._get_session_history,
            input_messages_key="question",
            history_messages_key="chat_history",
        )

    def _get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        if session_id not in self.history_store:
            self.history_store[session_id] = ChatMessageHistory()
        return self.history_store[session_id]

    def chat(self, user_input, session_id="user_main"):
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {user_input}")
        
        # 1. ë‹µë³€ ìƒì„± (ì—¬ê¸°ì„œ Spreading Activation ë°œìƒ)
        response = self.chain_with_history.invoke(
            {"question": user_input},
            config={"configurable": {"session_id": session_id}}
        )
        
        print(f"ğŸ‘©â€ğŸ¦° ë ˆë¯¸: {response}")

        # 2. ëŒ€í™” í›„ 'ê¸°ì–µ í˜•ì„±' í”„ë¡œì„¸ìŠ¤ (ìë™í™”)
        # (ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ë©´ ë” ì¢‹ì§€ë§Œ, ì—¬ê¸°ì„  ìˆœì°¨ ì²˜ë¦¬)
        self.brain.form_long_term_memory(user_input, response, self.llm)

# ==========================================
# [Main] ì‹¤í–‰ë¶€
# ==========================================
if __name__ == "__main__":
    # ë² í‹° ê¹¨ìš°ê¸°
    betty = BettyBot()

    # ì´ˆê¸° ê¸°ì–µì´ ì—†ë‹¤ë©´ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° í•˜ë‚˜ ì£¼ì… (ì²« ì‹¤í–‰ ì‹œ í•„ìš”)
    if not betty.brain.memory_data["contexts"]:
        print("ğŸŒ± ì´ˆê¸° ê¸°ì–µ ì‹¬ëŠ” ì¤‘...")
        betty.brain.form_long_term_memory(
            "ë‚˜ëŠ” ë¹„ ì˜¤ëŠ” ë‚  í•œê°•ì—ì„œ ì»µë¼ë©´ ë¨¹ëŠ” ê²Œ ì œì¼ ì¢‹ì•„.",
            "ì •ë§? ë‚˜ë„ ê·¸ë˜. ë¹—ì†Œë¦¬ ë“¤ìœ¼ë©´ì„œ ë¨¹ìœ¼ë©´ ê¿€ë§›ì´ì§€.", 
            betty.llm
        )

    # --- ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜ ---
    while True:
        user_text = input("\në§ì„ ê±°ì„¸ìš” (ì¢…ë£Œ: q): ")
        if user_text.lower() == 'q':
            break
        betty.chat(user_text)